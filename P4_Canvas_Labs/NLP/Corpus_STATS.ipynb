{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from nltk.collocations import *\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_ids = gutenberg.fileids()\n",
    "file_ids[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The Tragedie of Macbeth by William Shakespeare 1603]\n",
      "\n",
      "\n",
      "Actus Primus. Scoena Prima.\n",
      "\n",
      "Thunder and Lightning. Enter three Witches.\n",
      "\n",
      "  1. When shall we three meet againe?\n",
      "In Thunder, Lightning, or in Raine?\n",
      "  2. When the Hurley-burley's done,\n",
      "When the Battaile's lost, and wonne\n",
      "\n",
      "   3. That will be ere the set of Sunne\n",
      "\n",
      "   1. Where the place?\n",
      "  2. Vpon the Heath\n",
      "\n",
      "   3. There to meet with Macbeth\n",
      "\n",
      "   1. I come, Gray-Malkin\n",
      "\n",
      "   All. Padock calls anon: faire is foule, and foule is faire,\n",
      "Houer through \n"
     ]
    }
   ],
   "source": [
    "macbeth_text = gutenberg.raw(file_ids[-2])\n",
    "print(macbeth_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the words are hyphenated. If we just use basic tokenization, then it will split hyphenated words into individual tokens. There are also numbers that act as metadata about which witch is speaking -- we'll need to remove these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Data\n",
    "\n",
    "\n",
    "Looking at the text output above shows us a few things that we'll need to deal with during the preprocessing and tokenization steps -- specifically:\n",
    "\n",
    "Capitalization -- we'll need to lowercase all words.\n",
    "\n",
    "Apostrophes -- we'll need to write some basic regex in order to capture words that contain apostrophes as a single token. In the interest of time, a pattern has been provided for you. Use the following pattern: \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "\n",
    "Numbers -- We'll want to remove these, as they generally appear as stage direction to tell us which witch is speaking.\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "Store the pattern shown above in the appropriate variable\n",
    "Use nltk.regexp_tokenize() and pass in our text and the pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "macbeth_tokens_raw = nltk.regexp_tokenize(macbeth_text, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Tragedie',\n",
       " 'of',\n",
       " 'Macbeth',\n",
       " 'by',\n",
       " 'William',\n",
       " 'Shakespeare',\n",
       " 'Actus',\n",
       " 'Primus',\n",
       " 'Scoena']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_tokens_raw[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have our tokens, we need to lowercase them. In the cell below, use a list comprehension and the .lower() method on every word token in macbeth_tokens_raw. Store this inside macbeth_tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "macbeth_tokens = [w.lower() for w in macbeth_tokens_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've done some basic cleaning and tokenization, let's go ahead and create a Frequency Distribution to see the number of times each word is used in this play. This frequency distribution is an example of a Bag of Words, which you've worked with in previous labs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "macbeth_freqdist = FreqDist(macbeth_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 649),\n",
       " ('and', 545),\n",
       " ('to', 383),\n",
       " ('of', 338),\n",
       " ('i', 331),\n",
       " ('a', 241),\n",
       " ('that', 227),\n",
       " ('my', 203),\n",
       " ('you', 203),\n",
       " ('in', 199)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_freqdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that doesn't tell us very much! The top 10 most used words in macbeth are all Stop Words. They don't contain any interesting information, and essentially just act as the \"connective tissue\" between the words that really matter in any text. Let's try removing the stopwords and punctuation, and then creating another frequency distribution that contains only the important words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Stop Words and Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already imported the stopwords module. We can access all of the stopwords using the stopwords.words() method -- however, we don't want to use the whole thing, as this contains all stopwords in every language supported by NLTK. We don't need to check for and remove any Finnish or Japanese stop words, as this text is in English. To avoid unnecessarily long runtimes, we'll just use the English subset of stopwords by passing in the parameter \"english\" into stopwords.words().\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "\n",
    "Get all the 'english' stopwords from stopwords.words() and store them in the appropriate variable below. They will be stored as a list, by default\n",
    "\n",
    "We'll also want to remove all punctuation. Create a list version of string.punctuation and add it to our stopwords list\n",
    "\n",
    "Finally, we'll also remove numbers. Create a list that contains numbers 0-9 (as strings!), and add this to the stopwords list as well\n",
    "\n",
    "Use another list comprehension to get words out of macbeth_tokens as long as they are not in stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list+= list(string.punctuation)\n",
    "stopwords_list += ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "macbeth_words_stopped = [word for word in macbeth_tokens if word not in stopwords_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, let's create another frequency distribution using macbeth_words_stopped, and then inspect the top 50 most common words, to see if removing stopwords and punctuation has helped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('macb', 137),\n",
       " ('haue', 122),\n",
       " ('thou', 87),\n",
       " ('enter', 81),\n",
       " ('shall', 68),\n",
       " ('macbeth', 62),\n",
       " ('thee', 61),\n",
       " ('vpon', 58),\n",
       " ('macd', 58),\n",
       " ('yet', 57)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_stopped_freqdist = FreqDist(macbeth_words_stopped)\n",
    "macbeth_stopped_freqdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is definitely an improvement! You may be wondering why 'Macb' shows up as the number 1 most used token. If you inspect Macbeth on project gutenberg and search for 'Macb', you'll soon discover that the source text denotes Macb as stage direction for any line spoken by Macbeth's character. This means that 'Macb' is actually stage direction, meaning that under normal circumstances, we would need to ask ourselves if it is worth it to remove it or keep it. In the interest of time for this lab, we'll leave it be.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering Questions about our Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3441"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vocabulary Size\n",
    "# What is the size of the total vocabulary used in Macbeth, once all stopwords have been removed?\n",
    "\n",
    "len(macbeth_stopped_freqdist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized Word Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing the frequency with which each word is used is somewhat informative, but without the context of how many words are used in total, it doesn't tell us much. One way we can adjust for this is to use Normalized Word Frequency, which we can compute by dividing each word frequency by the total number of words.\n",
    "\n",
    "Compute this now in the cell below, and display the normalized word frequency for the top 50 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = sum(macbeth_stopped_freqdist.values())\n",
    "macbeth_top_50 = macbeth_stopped_freqdist.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word\t\t\tNormalized Frequency\n",
      "macb \t\t\t 0.01354\n",
      "haue \t\t\t 0.01206\n",
      "thou \t\t\t 0.008601\n",
      "enter \t\t\t 0.008008\n",
      "shall \t\t\t 0.006723\n",
      "macbeth \t\t\t 0.00613\n",
      "thee \t\t\t 0.006031\n",
      "vpon \t\t\t 0.005734\n",
      "macd \t\t\t 0.005734\n",
      "yet \t\t\t 0.005635\n",
      "thy \t\t\t 0.005536\n",
      "vs \t\t\t 0.005437\n",
      "come \t\t\t 0.005339\n",
      "king \t\t\t 0.005339\n",
      "hath \t\t\t 0.005141\n",
      "good \t\t\t 0.004844\n",
      "rosse \t\t\t 0.004844\n",
      "lady \t\t\t 0.004745\n",
      "would \t\t\t 0.004647\n",
      "time \t\t\t 0.004548\n",
      "like \t\t\t 0.004251\n",
      "say \t\t\t 0.003856\n",
      "doe \t\t\t 0.003757\n",
      "lord \t\t\t 0.003757\n",
      "make \t\t\t 0.003757\n",
      "tis \t\t\t 0.003658\n",
      "must \t\t\t 0.003559\n",
      "done \t\t\t 0.00346\n",
      "selfe \t\t\t 0.00346\n",
      "ile \t\t\t 0.00346\n",
      "feare \t\t\t 0.00346\n",
      "let \t\t\t 0.00346\n",
      "man \t\t\t 0.003361\n",
      "wife \t\t\t 0.003361\n",
      "night \t\t\t 0.003361\n",
      "banquo \t\t\t 0.003361\n",
      "well \t\t\t 0.003262\n",
      "know \t\t\t 0.003262\n",
      "one \t\t\t 0.003164\n",
      "great \t\t\t 0.003065\n",
      "see \t\t\t 0.003065\n",
      "may \t\t\t 0.003065\n",
      "exeunt \t\t\t 0.002966\n",
      "speake \t\t\t 0.002867\n",
      "sir \t\t\t 0.002867\n",
      "lenox \t\t\t 0.002768\n",
      "mine \t\t\t 0.00257\n",
      "vp \t\t\t 0.00257\n",
      "th \t\t\t 0.00257\n",
      "mal \t\t\t 0.002472\n"
     ]
    }
   ],
   "source": [
    "print('Word\\t\\t\\tNormalized Frequency')\n",
    "for w in macbeth_top_50:\n",
    "    normalized_freq = w[1] / words_count\n",
    "    print('{} \\t\\t\\t {:.4}'.format(w[0], normalized_freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing individual word frequencies is somewhat informative, but in practice, some of these tokens are actually parts of larger phrases that should be treated as a single unit. Let's create some bigrams, and see which combinations of words are most telling.\n",
    "\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* We'll begin by aliasing a particularly long method name to make it easier to call. Store nltk.collocations.BigramAssocMeasures() inside of the variable bigram_measures\n",
    "* Next, we'll need to create a finder. Pass macbeth_words_stopped into BigramCollocationFinder.from_words() and assign the result to macbeth_finder\n",
    "* Once we have a finder, we can use it to compute bigram scores, so we can see the combinations that occur most frequently. Call the macbeth_finder object's score_ngrams() method and pass in bigram_measures.raw_freq as the input\n",
    "* Display first 50 elements in the macbeth_scored list to see the 50 most common bigrams in macbeth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "macbeth_finder = BigramCollocationFinder.from_words(macbeth_words_stopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('enter', 'macbeth'), 0.0015818091942659417),\n",
       " (('exeunt', 'scena'), 0.0014829461196243204),\n",
       " (('thane', 'cawdor'), 0.0012852199703410777),\n",
       " (('knock', 'knock'), 0.0009886307464162135),\n",
       " (('lord', 'macb'), 0.0008897676717745922),\n",
       " (('thou', 'art'), 0.0008897676717745922),\n",
       " (('good', 'lord'), 0.0007909045971329708),\n",
       " (('haue', 'done'), 0.0007909045971329708),\n",
       " (('macb', 'haue'), 0.0007909045971329708),\n",
       " (('enter', 'lady'), 0.0006920415224913495),\n",
       " (('let', 'vs'), 0.0006920415224913495),\n",
       " (('macbeth', 'macb'), 0.0005931784478497281),\n",
       " (('enter', 'malcolme'), 0.0004943153732081067),\n",
       " (('enter', 'three'), 0.0004943153732081067),\n",
       " (('euery', 'one'), 0.0004943153732081067),\n",
       " (('macb', 'ile'), 0.0004943153732081067),\n",
       " (('macb', 'thou'), 0.0004943153732081067),\n",
       " (('make', 'vs'), 0.0004943153732081067),\n",
       " (('mine', 'eyes'), 0.0004943153732081067),\n",
       " (('mine', 'owne'), 0.0004943153732081067),\n",
       " (('scena', 'secunda'), 0.0004943153732081067),\n",
       " (('three', 'witches'), 0.0004943153732081067),\n",
       " (('thy', 'selfe'), 0.0004943153732081067),\n",
       " (('worthy', 'thane'), 0.0004943153732081067),\n",
       " (('would', 'haue'), 0.0004943153732081067),\n",
       " (('borne', 'woman'), 0.0003954522985664854),\n",
       " (('come', 'come'), 0.0003954522985664854),\n",
       " (('enter', 'banquo'), 0.0003954522985664854),\n",
       " (('enter', 'king'), 0.0003954522985664854),\n",
       " (('enter', 'macduffe'), 0.0003954522985664854),\n",
       " (('enter', 'rosse'), 0.0003954522985664854),\n",
       " (('haile', 'king'), 0.0003954522985664854),\n",
       " (('haile', 'macbeth'), 0.0003954522985664854),\n",
       " (('hath', 'made'), 0.0003954522985664854),\n",
       " (('haue', 'seene'), 0.0003954522985664854),\n",
       " (('macb', 'bring'), 0.0003954522985664854),\n",
       " (('macbeth', 'macbeth'), 0.0003954522985664854),\n",
       " (('malcolme', 'donalbaine'), 0.0003954522985664854),\n",
       " (('old', 'man'), 0.0003954522985664854),\n",
       " (('rosse', 'angus'), 0.0003954522985664854),\n",
       " (('scena', 'prima'), 0.0003954522985664854),\n",
       " (('see', 'thee'), 0.0003954522985664854),\n",
       " (('shew', 'shew'), 0.0003954522985664854),\n",
       " (('sir', 'macb'), 0.0003954522985664854),\n",
       " (('ten', 'thousand'), 0.0003954522985664854),\n",
       " (('tertia', 'enter'), 0.0003954522985664854),\n",
       " (('thy', 'face'), 0.0003954522985664854),\n",
       " (('woman', 'borne'), 0.0003954522985664854),\n",
       " (('would', 'make'), 0.0003954522985664854),\n",
       " (('alarums', 'enter'), 0.00029658922392486405)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_scored = macbeth_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "macbeth_scored[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look a bit more interesting. We can see here that some of the most common ones are stage directions, such as 'Enter Macbeth' and 'Exeunt Scena', while others seem to be common phrases used in the play.\n",
    "\n",
    "To wrap up our initial examination of Macbeth, let's end by calculating Mutual Information Scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Mutual Information Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate mutual information scores, we'll need to first create a frequency filter, so that we only examine bigrams that occur more than a set number of times -- for our purposes, we'll set this limit to 5.\n",
    "\n",
    "In NLTK, mutual information is often referred to as pmi, for Pointwise Mutual Information. Calculating PMI scores works much the same way that we created bigrams, with a few notable differences.\n",
    "\n",
    "\n",
    "In the cell below:\n",
    "\n",
    "* We'll start by creating another finder for pmi. Pass macbeth_words_stopped as the input to BigramCollocationFinder.from_words(). Store this is the variable macbeth_pmi_finder\n",
    "* Once we have our finder, we'll need to apply our frequency filter. Call macbeth_pmi_finder's apply_freq_filter and pass in the number 5 as the input\n",
    "* Now, we can use the finder to calculate pmi scores. Use the pmi finder's .score_ngrams() method, and pass in bigram_measures.pmi as the argument. Store this in macbeth_pmi_scored\n",
    "* Examine the first 50 elements in macbeth_pmi_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('three', 'witches'), 8.925697076191916),\n",
       " (('scena', 'secunda'), 8.844777080808349),\n",
       " (('knock', 'knock'), 8.62613679433301),\n",
       " (('thane', 'cawdor'), 7.968474805033251),\n",
       " (('exeunt', 'scena'), 7.844777080808349),\n",
       " (('mine', 'eyes'), 7.46626545755462),\n",
       " (('worthy', 'thane'), 6.982280604558284),\n",
       " (('mine', 'owne'), 6.838234234941577),\n",
       " (('euery', 'one'), 6.626136794333009),\n",
       " (('thou', 'art'), 5.861265203596917),\n",
       " (('enter', 'malcolme'), 5.585847073307292),\n",
       " (('enter', 'three'), 5.585847073307292),\n",
       " (('good', 'lord'), 5.441571341886851),\n",
       " (('let', 'vs'), 5.2009208910336255),\n",
       " (('enter', 'macbeth'), 5.0101623861741444),\n",
       " (('thy', 'selfe'), 4.689498855330438),\n",
       " (('make', 'vs'), 4.596849567364764),\n",
       " (('haue', 'done'), 4.2441883449377915),\n",
       " (('enter', 'lady'), 4.186751117897471),\n",
       " (('lord', 'macb'), 4.128174104483847),\n",
       " (('macb', 'ile'), 3.3988216944275145),\n",
       " (('would', 'haue'), 3.1408106050924847),\n",
       " (('macbeth', 'macb'), 2.836942806819401),\n",
       " (('macb', 'haue'), 2.2754392789222315),\n",
       " (('macb', 'thou'), 2.0851612155237547)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macbeth_pmi_finder = BigramCollocationFinder.from_words(macbeth_words_stopped)\n",
    "macbeth_pmi_finder.apply_freq_filter(5)\n",
    "macbeth_pmi_scored = macbeth_pmi_finder.score_ngrams(bigram_measures.pmi)\n",
    "macbeth_pmi_scored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
